{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "n_samples = 2000\n",
    "n_features = 1000\n",
    "n_components = 10\n",
    "n_top_words = 20\n",
    "\n",
    "\n",
    "def print_top_words(model, feature_names, n_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        message = \"Topic #%d: \" % topic_idx\n",
    "        message += \" \".join([feature_names[i]\n",
    "                             for i in topic.argsort()[:-n_top_words - 1:-1]])\n",
    "        print(message)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "done in 3.100s.\n"
     ]
    }
   ],
   "source": [
    "# Load the 20 newsgroups dataset and vectorize it. We use a few heuristics\n",
    "# to filter out useless terms early on: the posts are stripped of headers,\n",
    "# footers and quoted replies, and common English words, words occurring in\n",
    "# only one document or in at least 95% of the documents are removed.\n",
    "\n",
    "print(\"Loading dataset...\")\n",
    "t0 = time()\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1,\n",
    "                             remove=('headers', 'footers', 'quotes'))\n",
    "data_samples = dataset.data[:n_samples]\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf-idf features for NMF...\n",
      "done in 0.665s.\n"
     ]
    }
   ],
   "source": [
    "# Use tf-idf features.\n",
    "print(\"Extracting tf-idf features for NMF...\")\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=2,\n",
    "                                   max_features=n_features,\n",
    "                                   stop_words='english')\n",
    "t0 = time()\n",
    "tfidf = tfidf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting tf features for LDA...\n",
      "done in 0.655s.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use tf (raw term count) features for LDA.\n",
    "print(\"Extracting tf features for LDA...\")\n",
    "tf_vectorizer = CountVectorizer(max_df=0.95, min_df=2,\n",
    "                                max_features=n_features,\n",
    "                                stop_words='english')\n",
    "t0 = time()\n",
    "tf = tf_vectorizer.fit_transform(data_samples)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting LDA models with tf features, n_samples=2000 and n_features=1000...\n",
      "done in 2.879s.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting LDA models with tf features, \"\n",
    "      \"n_samples=%d and n_features=%d...\"\n",
    "      % (n_samples, n_features))\n",
    "lda = LatentDirichletAllocation(n_components=n_components, max_iter=5,\n",
    "                                learning_method='online',\n",
    "                                learning_offset=50.,\n",
    "                                random_state=0)\n",
    "t0 = time()\n",
    "lda.fit(tf)\n",
    "print(\"done in %0.3fs.\" % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topics in LDA model:\n",
      "Topic #0: edu com mail send graphics ftp pub available contact university list faq ca information cs 1993 program sun uk mit\n",
      "Topic #1: don like just know think ve way use right good going make sure ll point got need really time doesn\n",
      "Topic #2: christian think atheism faith pittsburgh new bible radio games alt lot just religion like book read play time subject believe\n",
      "Topic #3: drive disk windows thanks use card drives hard version pc software file using scsi help does new dos controller 16\n",
      "Topic #4: hiv health aids disease april medical care research 1993 light information study national service test led 10 page new drug\n",
      "Topic #5: god people does just good don jesus say israel way life know true fact time law want believe make think\n",
      "Topic #6: 55 10 11 18 15 team game 19 period play 23 12 13 flyers 20 25 22 17 24 16\n",
      "Topic #7: car year just cars new engine like bike good oil insurance better tires 000 thing speed model brake driving performance\n",
      "Topic #8: people said did just didn know time like went think children came come don took years say dead told started\n",
      "Topic #9: key space law government public use encryption earth section security moon probe enforcement keys states lunar military crime surface technology\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nTopics in LDA model:\")\n",
    "tf_feature_names = tf_vectorizer.get_feature_names()\n",
    "print_top_words(lda, tf_feature_names, n_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_word_distributions = np.array([row / np.sum(row)\n",
    "                                     for row in lda.components_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(topic_word_distributions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying the top 20 words per topic and their probabilities within the topic...\n",
      "\n",
      "[ Topic 0 ]\n",
      "edu : 0.05201295519414276\n",
      "com : 0.02330146798243792\n",
      "mail : 0.018757677200607305\n",
      "send : 0.017853034313902207\n",
      "graphics : 0.01595098191416805\n",
      "ftp : 0.014210395836814476\n",
      "pub : 0.013452740553974222\n",
      "available : 0.011952686203526925\n",
      "contact : 0.011686121720430657\n",
      "university : 0.010583275527159331\n",
      "list : 0.009855197508471462\n",
      "faq : 0.00957636727342271\n",
      "ca : 0.009509668275976258\n",
      "information : 0.009114667086822981\n",
      "cs : 0.008586300586097028\n",
      "1993 : 0.007287622462962467\n",
      "program : 0.007127524628490156\n",
      "sun : 0.007015770896936095\n",
      "uk : 0.006898763186196177\n",
      "mit : 0.006855242806574545\n",
      "\n",
      "[ Topic 1 ]\n",
      "don : 0.02252631873000027\n",
      "like : 0.01920069484451604\n",
      "just : 0.016172949248556064\n",
      "know : 0.014221316124639132\n",
      "think : 0.013672203283395329\n",
      "ve : 0.012894625646772533\n",
      "way : 0.012293395990878963\n",
      "use : 0.010477578387284938\n",
      "right : 0.0104737325409385\n",
      "good : 0.010437073090386382\n",
      "going : 0.010000710949059645\n",
      "make : 0.009012636990721813\n",
      "sure : 0.00854899917065906\n",
      "ll : 0.008421857190456407\n",
      "point : 0.007740498647604111\n",
      "got : 0.007739467458600745\n",
      "need : 0.007157696679956572\n",
      "really : 0.007051879181552564\n",
      "time : 0.006808575881926602\n",
      "doesn : 0.006753129655481832\n",
      "\n",
      "[ Topic 2 ]\n",
      "christian : 0.03246905623669129\n",
      "think : 0.03053133570119102\n",
      "atheism : 0.023140364283488913\n",
      "faith : 0.019385890447121114\n",
      "pittsburgh : 0.018174961815483406\n",
      "new : 0.013547452168322791\n",
      "bible : 0.012346358804735386\n",
      "radio : 0.012129767290121447\n",
      "games : 0.011899652878665942\n",
      "alt : 0.011750142035273269\n",
      "lot : 0.011145137683166459\n",
      "just : 0.010765512537948272\n",
      "religion : 0.010116957728370858\n",
      "like : 0.01010566577344118\n",
      "book : 0.010076011879763853\n",
      "read : 0.009959037749786088\n",
      "play : 0.00967692051874182\n",
      "time : 0.00949214303703619\n",
      "subject : 0.009481177889449353\n",
      "believe : 0.009433693676928583\n",
      "\n",
      "[ Topic 3 ]\n",
      "drive : 0.015516269149139027\n",
      "disk : 0.013378607025368227\n",
      "windows : 0.013345319294017515\n",
      "thanks : 0.011536459032654694\n",
      "use : 0.010955628400982118\n",
      "card : 0.010884922758572904\n",
      "drives : 0.010398867753195946\n",
      "hard : 0.01001703918928586\n",
      "version : 0.009621048127250226\n",
      "pc : 0.009596673400556999\n",
      "software : 0.009314515114636233\n",
      "file : 0.009251301945711252\n",
      "using : 0.008947799625304015\n",
      "scsi : 0.008090028420419941\n",
      "help : 0.0076129234769485715\n",
      "does : 0.0075349963598619245\n",
      "new : 0.007354339295584864\n",
      "dos : 0.007339615728733452\n",
      "controller : 0.007227521590175326\n",
      "16 : 0.007192320787630609\n",
      "\n",
      "[ Topic 4 ]\n",
      "hiv : 0.04538636522325112\n",
      "health : 0.04382437724813918\n",
      "aids : 0.03472532748225219\n",
      "disease : 0.0320239069741685\n",
      "april : 0.029982016318375285\n",
      "medical : 0.023207250065698713\n",
      "care : 0.022038766563193347\n",
      "research : 0.018011406473398012\n",
      "1993 : 0.01692268417139178\n",
      "light : 0.016334955295514347\n",
      "information : 0.014808652818737566\n",
      "study : 0.014198492617171213\n",
      "national : 0.014183231569545818\n",
      "service : 0.014144282843589623\n",
      "test : 0.0138112729723715\n",
      "led : 0.013240263307603039\n",
      "10 : 0.011543443309850212\n",
      "page : 0.011012917207197407\n",
      "new : 0.010445494935250382\n",
      "drug : 0.010305987471381195\n",
      "\n",
      "[ Topic 5 ]\n",
      "god : 0.028927387630506455\n",
      "people : 0.02078742049554546\n",
      "does : 0.016518880733500812\n",
      "just : 0.01238364777658631\n",
      "good : 0.010553996990709373\n",
      "don : 0.010154237011733623\n",
      "jesus : 0.009258791410284862\n",
      "say : 0.008802591781035355\n",
      "israel : 0.008539410267511677\n",
      "way : 0.008447843935264035\n",
      "life : 0.008404195494329247\n",
      "know : 0.0080050569426588\n",
      "true : 0.007449137793979688\n",
      "fact : 0.007436820947732358\n",
      "time : 0.007424829669905558\n",
      "law : 0.007322407176817605\n",
      "want : 0.007216852841411078\n",
      "believe : 0.007209266153507557\n",
      "make : 0.00719007978083258\n",
      "think : 0.007118025546269622\n",
      "\n",
      "[ Topic 6 ]\n",
      "55 : 0.03624489908653222\n",
      "10 : 0.03102586673131415\n",
      "11 : 0.02337113285126161\n",
      "18 : 0.01733462582096894\n",
      "15 : 0.016549562455125743\n",
      "team : 0.016477099193474808\n",
      "game : 0.016265775698587136\n",
      "19 : 0.015648430246505828\n",
      "period : 0.015464257311821817\n",
      "play : 0.014637002583728322\n",
      "23 : 0.014502786903071415\n",
      "12 : 0.014371251133076576\n",
      "13 : 0.0143199346628954\n",
      "flyers : 0.01417992660572723\n",
      "20 : 0.013901917876087213\n",
      "25 : 0.01378460341224644\n",
      "22 : 0.013465639297657676\n",
      "17 : 0.013159126337140581\n",
      "24 : 0.012889442076335708\n",
      "16 : 0.012869072242481225\n",
      "\n",
      "[ Topic 7 ]\n",
      "car : 0.033147878167394486\n",
      "year : 0.01667497971493922\n",
      "just : 0.01358064725959472\n",
      "cars : 0.012632691772703044\n",
      "new : 0.012364674116600721\n",
      "engine : 0.011969428392882099\n",
      "like : 0.011734922267757155\n",
      "bike : 0.011617683897852976\n",
      "good : 0.011018187476231497\n",
      "oil : 0.010992962541528798\n",
      "insurance : 0.010942372330143155\n",
      "better : 0.009306154860056978\n",
      "tires : 0.00845356753011771\n",
      "000 : 0.007799358058219855\n",
      "thing : 0.007360395338669547\n",
      "speed : 0.007261139034026603\n",
      "model : 0.007259304341314295\n",
      "brake : 0.006948373401909818\n",
      "driving : 0.00688470039114346\n",
      "performance : 0.00675497235677965\n",
      "\n",
      "[ Topic 8 ]\n",
      "people : 0.028349402183412295\n",
      "said : 0.017361347275918357\n",
      "did : 0.01275941821551866\n",
      "just : 0.012189912307124713\n",
      "didn : 0.012187553150987957\n",
      "know : 0.01135599210450992\n",
      "time : 0.01082868787105864\n",
      "like : 0.010657804867414317\n",
      "went : 0.008708429294101806\n",
      "think : 0.008481283843052101\n",
      "children : 0.007353767400852342\n",
      "came : 0.00687497388033297\n",
      "come : 0.006836683596194964\n",
      "don : 0.006799498061834065\n",
      "took : 0.0066033382764478605\n",
      "years : 0.006423153681972156\n",
      "say : 0.006269409693950504\n",
      "dead : 0.005768094022600942\n",
      "told : 0.005601205260814976\n",
      "started : 0.005597655068715208\n",
      "\n",
      "[ Topic 9 ]\n",
      "key : 0.021873213250490034\n",
      "space : 0.02115967024609965\n",
      "law : 0.014953148884934273\n",
      "government : 0.013066710525171842\n",
      "public : 0.012551211642513374\n",
      "use : 0.011222350615433713\n",
      "encryption : 0.010389261269872396\n",
      "earth : 0.01013981172250363\n",
      "section : 0.009617314944609118\n",
      "security : 0.009480113526455653\n",
      "moon : 0.008424825273353715\n",
      "probe : 0.007515303452619629\n",
      "enforcement : 0.007461716182060758\n",
      "keys : 0.0074613488037655945\n",
      "states : 0.007373454932830266\n",
      "lunar : 0.00692801420427191\n",
      "military : 0.006836891385254655\n",
      "crime : 0.006577038059926774\n",
      "surface : 0.006569127142455634\n",
      "technology : 0.006388304229188106\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Displaying the top', n_top_words, 'words per topic and their probabilities within the topic...')\n",
    "print()\n",
    "\n",
    "for topic_idx in range(n_components):\n",
    "    print('[ Topic', topic_idx, ']')\n",
    "    sort_indices = np.argsort(topic_word_distributions[topic_idx])[::-1]  # highest prob to lowest prob word indices\n",
    "    for rank in range(n_top_words):\n",
    "        word_idx = sort_indices[rank]\n",
    "        print(tf_vectorizer.get_feature_names()[word_idx], ':', topic_word_distributions[topic_idx, word_idx])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
